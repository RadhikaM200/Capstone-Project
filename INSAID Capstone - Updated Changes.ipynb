{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium\n",
    "# !pip install geocoder\n",
    "# !pip install chart_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "# !pip install geopy\n",
    "# !pip install git+https://github.com/BoseCorp/py-googletrans.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector as conn\n",
    "import statistics\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio.plotly as py\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%18f' %x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:/Users/user/INSAID/Project/events_data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "#this is to connect to mysql database\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"cpanel.insaid.co\",\n",
    "  user=\"student\",\n",
    "  password=\"student\",\n",
    "  database=\"Capstone1\"\n",
    ")\n",
    "\n",
    "#declaring a cursor variable of database to hold data of a specific table\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#mycursor variable holds the entire data from gender_age_train table\n",
    "mycursor.execute(\"SELECT * FROM gender_age_train\")\n",
    "\n",
    "#using fetchall function to get data from cursor and populating in another variable\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "#inserting table data from variable into a dataframe\n",
    "gender_age_train_df = pd.DataFrame(myresult,columns=['device_id','Gender','Age','Age_Group'])\n",
    "\n",
    "#mycursor variable holds the entire data from phone_brand_device_model table\n",
    "mycursor.execute(\"SELECT * FROM phone_brand_device_model\")\n",
    "\n",
    "#using fetchall function to get data from cursor and populating in another variable, this overwrites data from previous load\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "#inserting table data from variable into a dataframe\n",
    "phone_brand_device_model_df = pd.DataFrame(myresult,columns=['device_id','Brand','Model'])\n",
    "\n",
    "#printing sample data from the dataframes\n",
    "print(\"Data from Gender_Age_Train Table\")\n",
    "print(\"---------------------------------\")\n",
    "print(gender_age_train_df.head())\n",
    "print(\"**************************************\")\n",
    "print(\"\\n\")\n",
    "print(\"Data from Phone_Brand_Device_Model Table\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(phone_brand_device_model_df.head())\n",
    "print(\"**************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_age_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_brand_device_model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Checking for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_age_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_brand_device_model_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Imputing missing values in \"device_id\" in dataset** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing list of unique latitude values for the missing deviceid records.\n",
    "latlon_df = dataset[dataset.device_id.isnull()]['latitude'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#For imputing Missing values in DeviceId, we've fetched unique latitude for the missing deviceid and found that for that latitude there is only one deviceid throughout the dataset.\n",
    "#Hence, we'll be imputing missing deviceid values by replacing the deviceid which the same latitudeid shares\n",
    "for latitude in latlon_df:\n",
    "    devid = list(dataset[(dataset.latitude == latitude) & (dataset.device_id.notnull())].device_id)[0]\n",
    "    print('Latitude is {} for DeviceID: {}'.format(latitude, devid))\n",
    "    dataset.loc[dataset.latitude == latitude, 'device_id'] = devid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Imputing Missing values in \"State\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State wise count before imputing missing state values\n",
    "dataset[(dataset.state == 'WestBengal') | (dataset.state == 'Karnataka') | (dataset.state == 'Gujarat') | (dataset.state == 'Bihar') | (dataset.state == 'Punjab') | (dataset.state == 'Kerala')]['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For imputing the missing values in State, we'll first find all their respecitve city values\n",
    "city_list = list(dataset[dataset.state.isnull()].city.unique())\n",
    "city_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Will loop over the above cities list, fetch it's relevant state value where state value is not null and then populate that to all the records where city value is matching with the city value from our list\n",
    "for city in city_list:\n",
    "    state = list(dataset[(dataset.city == city) & (dataset.state.notnull())].state)[0]\n",
    "    print(\"State is {} for City {}\".format(state,city))\n",
    "    dataset.loc[dataset.city == city, 'state'] = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values in \"Latitude & Longitude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking if latitude and longitude are null for the same set of records\n",
    "dataset[(dataset.latitude.isnull()) & (dataset.longitude.isnull())].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the final count for the records where lat and long both are null is equal to the total number of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values in events_data_set\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Taking the ids and checking the counts for city and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device_id = dataset[(dataset.latitude.isnull()) & (dataset.longitude.isnull())]['device_id'].unique()\n",
    "\n",
    "print(\"Total records : \", len(device_id))\n",
    "print(\"---------------------------------------------------\")\n",
    "for ids in device_id:\n",
    "    print(\"The total count for device_id : {} is : {}\".format(ids, dataset[dataset.device_id == ids][['city','state']].nunique()))\n",
    "# dataset[dataset.device_id == -8790560034584249344.000000][['city','state']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each device_id belongs to one pair of city and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset.latitude.isnull()) & (dataset.longitude.isnull())].sort_values(['device_id'])['device_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if lat and long values exist for atleast one record for the particular device_id\n",
    "temp = dataset[~(dataset.latitude.isnull() & dataset.longitude.isnull())]\n",
    "for ids in device_id:\n",
    "    print(\"The device_id : {} lat : {} long : {}\".format(ids, temp[temp.device_id == ids].latitude[:1].values[0], temp[temp.device_id == ids].longitude[:1].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **It seems that for all device_id lat and long values are already present in some of records, these can be used to impute the missing values at other places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values\n",
    "\n",
    "for ids in device_id:\n",
    "    dataset.loc[dataset.device_id==ids, 'latitude'] = temp[temp.device_id == ids].latitude[:1].values[0]\n",
    "    dataset.loc[dataset.device_id==ids, 'longitude'] = temp[temp.device_id == ids].longitude[:1].values[0]\n",
    "#     print(\"The device_id : {} lat : {} long : {}\".format(ids, temp[temp.device_id == ids].latitude[:1].values[0], temp[temp.device_id == ids].longitude[:1].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Handling non-english characters in phone_brand_device_model_df dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender_age_train_df.brand.value_counts()\n",
    "phone_brand_device_model_df.Brand.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Updating Mobile Brand for couple of Brands where English mapping is provided by INSAID\n",
    "# lang_dict = {'华为' : 'Huawei', \n",
    "#              '小米' : 'Xiaomi',\n",
    "#              '三星' : 'Samsung',\n",
    "#              '魅族' : 'Meizu',\n",
    "#              '酷派' : 'Coolpad',\n",
    "#              '乐视' : 'LeEco',\n",
    "#              '联想' : 'Lenovo'}\n",
    "\n",
    "# phone_brand_device_model_df.Brand = phone_brand_device_model_df.Brand.apply(lambda x: lang_dict.setdefault(x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#To replace Non-English Characters in Brand column of phone_brand_device_model dataframe\n",
    "#We are using a dictionary found online which has mapping for all Non-English Characters to English \n",
    "phone_brands_mapping = {\"三星\": \"samsung\",\"天语\": \"Ktouch\", \"海信\": \"hisense\", \"联想\": \"lenovo\", \"欧比\": \"obi\",\n",
    "                                \"爱派尔\": \"ipair\", \"努比亚\": \"nubia\", \"优米\": \"youmi\", \"朵唯\": \"dowe\", \"黑米\": \"heymi\",\n",
    "                                \"锤子\": \"hammer\", \"酷比魔方\": \"koobee\", \"美图\": \"meitu\", \"尼比鲁\": \"nibilu\", \"一加\": \"oneplus\",\n",
    "                                \"优购\": \"yougo\", \"诺基亚\": \"nokia\", \"糖葫芦\": \"candy\", \"中国移动\": \"ccmc\", \"语信\": \"yuxin\",\n",
    "                                \"基伍\": \"kiwu\", \"青橙\": \"greeno\", \"华硕\": \"asus\", \"夏新\": \"panasonic\", \"维图\": \"weitu\",\n",
    "                                \"艾优尼\": \"aiyouni\", \"摩托罗拉\": \"moto\", \"乡米\": \"xiangmi\", \"米奇\": \"micky\", \"大可乐\": \"bigcola\",\n",
    "                                \"沃普丰\": \"wpf\", \"神舟\": \"hasse\", \"摩乐\": \"mole\", \"飞秒\": \"fs\", \"米歌\": \"mige\", \"富可视\": \"fks\",\n",
    "                                \"德赛\": \"desci\", \"梦米\": \"mengmi\", \"乐视\": \"lshi\", \"小杨树\": \"smallt\", \"纽曼\": \"newman\",\n",
    "                                \"邦华\": \"banghua\", \"E派\": \"epai\", \"易派\": \"epai\", \"普耐尔\": \"pner\", \"欧新\": \"ouxin\", \"西米\": \"ximi\",\n",
    "                                \"海尔\": \"haier\", \"波导\": \"bodao\", \"糯米\": \"nuomi\", \"唯米\": \"weimi\", \"酷珀\": \"kupo\", \"谷歌\": \"google\",\n",
    "                                \"昂达\": \"ada\", \"聆韵\": \"lingyun\", \"小米\": \"Xiaomi\", \"华为\": \"Huawei\", \"魅族\": \"Meizu\", \"中兴\": \"ZTE\",\n",
    "                                \"酷派\": \"Coolpad\", \"金立\": \"Gionee\",\n",
    "                                \"索尼\" : \"Sony\", \"欧博信\" : \"Opssom\", \"奇酷\" : \"Qiku\",\n",
    "                                \"酷比\" : \"CUBE\", \"康佳\" : \"Konka\", \"亿通\" : \"Yitong\", \"金星数码\" : \"JXD\", \"至尊宝\" : \"Monkey King\",\n",
    "                                \"百立丰\" : \"Hundred Li Feng\", \"贝尔丰\" : \"Bifer\", \"百加\" : \"Bacardi\", \"诺亚信\" : \"Noain\", \n",
    "                                \"广信\" : \"Kingsun\", \"世纪天元\" : \"Ctyon\", \"青葱\" : \"Cong\", \"果米\" : \"Taobao\", \"斐讯\" : \"Phicomm\",\n",
    "                                \"长虹\" : \"Changhong\", \"欧奇\" : \"Oukimobile\", \"先锋\" : \"XFPLAY\", \"台电\" : \"Teclast\", \"大Q\" : \"Daq\",\n",
    "                                \"蓝魔\" : \"Ramos\", \"奥克斯\" : \"AUX\", \"索尼\" : \"Sony\", \"欧博信\" : \"Opssom\", \"奇酷\" : \"Qiku\",\n",
    "                                \"酷比\" : \"CUBE\", \"康佳\" : \"Konka\", \"亿通\" : \"Yitong\", \"金星数码\" : \"JXD\", \"至尊宝\" : \"Monkey King\",\n",
    "                                \"百立丰\" : \"Hundred Li Feng\", \"贝尔丰\" : \"Bifer\", \"百加\" : \"Bacardi\", \"诺亚信\" : \"Noain\",\n",
    "                                \"广信\" : \"Kingsun\", \"世纪天元\" : \"Ctyon\", \"青葱\" : \"Cong\", \"果米\" : \"Taobao\", \"斐讯\" : \"Phicomm\",\n",
    "                                \"长虹\" : \"Changhong\", \"欧奇\" : \"Oukimobile\", \"先锋\" : \"XFPLAY\", \"台电\" : \"Teclast\", \"大Q\" : \"Daq\", \n",
    "                                \"蓝魔\" : \"Ramos\", \"奥克斯\" : \"AUX\", \"飞利浦\": \"Philips\", \"智镁\": \"Zhimei\", \"惠普\": \"HP\",\n",
    "                                \"原点\": \"Origin\", \"戴尔\": \"Dell\", \"碟米\": \"Diemi\", \"西门子\": \"Siemens\", \"亚马逊\": \"Amazon\",\n",
    "                                \"宏碁\": \"Acer\", \"E人E本\":\"Eben\",\n",
    "                                '世纪星': \"UB1\", '丰米': \"UB2\", '优语':'UB3', '凯利通': \"UB4\", '唯比': \"UB5\", '嘉源': \"UB6\",\n",
    "                                 '大显': \"UB7\", '天宏时代': \"UB8\", '宝捷讯': 'UB9','帷幄': 'UB10', '德卡诺': 'UB11',\n",
    "                                '恒宇丰': 'UB12', '本为': 'UB13', '极米': 'UB14', '欧乐迪': 'UB15', '欧乐酷': 'UB16',\n",
    "                                '欧沃': 'UB17', '瑞米': 'UB18', '瑞高': 'UB19', '白米': 'UB20', '虾米': 'UB21', '赛博宇华': 'UB22',\n",
    "                                '首云': 'UB23', '鲜米': 'UB24'}\n",
    "\n",
    "phone_brand_device_model_df.Brand = phone_brand_device_model_df.Brand.apply(lambda x: phone_brands_mapping.setdefault(x.strip(),x.strip()))\n",
    "print(\"\\n After mapping Non-English Characters in Brand: \\n\",phone_brand_device_model_df.Brand.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Filtering out the initial dataFrame (dataset) to keep values of only 6 concerned States**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered = dataset[(dataset.state == 'WestBengal') | (dataset.state == 'Karnataka') | (dataset.state == 'Gujarat') |  (dataset.state == 'Bihar') | \n",
    "         (dataset.state =='Punjab') | (dataset.state == 'Kerala')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Checking if the lat and long values are proper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(dataset_filtered, lat=\"latitude\", lon=\"longitude\", zoom=3,hover_name='city')\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")  # <== Using Mapbox\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Merging Dataframes on device_id to create a single dataframe which contains customer age, gender, mobile brand and model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining events dataset of 6 states & gender_age dataset on device_id\n",
    "device_data_df=pd.merge(dataset_filtered,gender_age_train_df,on='device_id',how='inner') #performing inner join to fetch only matching device_id records\n",
    "device_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining phone_brand_model dataset with gender_age dataset and events dataset for 6 states on device_id\n",
    "device_data_df=pd.merge(device_data_df,phone_brand_device_model_df,on='device_id',how='inner') #performing inner join to fetch only matching device_id records\n",
    "device_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cols in device_data_df.columns:\n",
    "#     print(cols)\n",
    "#     print(device_data_df.eval(cols).unique())\n",
    "#     print(\"--------------------\")\n",
    "\n",
    "\n",
    "device_data_df.columns\n",
    "device_data_df.Age_Group.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_data_df.eval(list(cols)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = str('abc')\n",
    "a\n",
    "eval(\"[str(a)]\")\n",
    "eval(\"[a]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_data_df.eval(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.device_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Analysing the dataset. Checking for null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We have missing values:**\n",
    "   - device_id = 48\n",
    "   - longitude = 42\n",
    "   - latitude  = 42\n",
    "   \n",
    "Notes : Cities and States are not missing for any of the records, hence, Lat and Long values can be derived. So, these values are not a problem, but, Device_ids are missing, this could subsequently lead to loss of info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dealing with device_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The percentage of missing values : {:0.2f}%\".format(dataset_filtered.device_id.isna().sum()/dataset_filtered.event_id.count()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This is a negligible percentage of missing values to deal with. We can safely drop it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered = dataset_filtered.dropna(subset=['device_id'], how=\"any\")\n",
    "\n",
    "dataset_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dealing with missing Lat and Long values**\n",
    "\n",
    "Values having probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset_filtered[dataset_filtered.longitude.isna() & dataset_filtered.latitude.isna()][['device_id', 'city', 'state', 'event_id']].sort_values('device_id')\n",
    "\n",
    "print(\"The total records displayed below: {}\".format(len(a)))\n",
    "print(\"*******************************************************\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered[dataset_filtered.longitude.isna() & dataset_filtered.latitude.isna()]['device_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Basic Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset_filtered[(dataset_filtered.city == 'Araria') & (dataset_filtered.state == 'Bihar')]\n",
    "print(\"The mean value for Araria, Bihar : Latitude = {0:0.6f},  Longitutde = {1:0.6f}\".format(statistics.mean(a['latitude'].dropna()),\n",
    "                                                                                  statistics.mean(a['longitude'].dropna())))\n",
    "print()\n",
    "a = dataset_filtered[(dataset_filtered.city == 'Moga') & (dataset_filtered.state == 'Punjab')]\n",
    "print(\"The mean value for Araria, Bihar : Latitude = {0:0.6f},  Longitutde = {1:0.6f}\".format(statistics.mean(a['latitude'].dropna()),\n",
    "                                                                                  statistics.mean(a['longitude'].dropna())))\n",
    "\n",
    "print()\n",
    "a = dataset_filtered[(dataset_filtered.city == 'Bagaha') & (dataset_filtered.state == 'Bihar')]\n",
    "print(\"The mean value for Araria, Bihar : Latitude = {0:0.6f},  Longitutde = {1:0.6f}\".format(statistics.mean(a['latitude'].dropna()),\n",
    "                                                                                  statistics.mean(a['longitude'].dropna())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the missing values with mean of all available values\n",
    "long = {'Araria' : 87.578218,\n",
    "       'Moga' : 75.189438,\n",
    "        'Bagaha' : 84.147561}\n",
    "\n",
    "lat = {'Araria' : 26.209851,\n",
    "       'Moga' : 30.895271,\n",
    "       'Bagaha' : 27.163957}\n",
    "\n",
    "# dataset_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dataset_filtered[dataset_filtered.longitude.isna() & dataset_filtered.latitude.isna()]['city']\n",
    "a = temp.apply(lambda x: lat[x])\n",
    "\n",
    "for indices in a.index:\n",
    "    dataset_filtered.loc[indices, 'latitude'] = a[indices]\n",
    "    \n",
    "a = temp.apply(lambda x: long[x])\n",
    "\n",
    "for indices in a.index:\n",
    "    dataset_filtered.loc[indices, 'longitude'] = a[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.Marker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # geocode address and place marker on map\n",
    "\n",
    "# # map\n",
    "# map_zoo = folium.Map(location=[32.744524, -117.150302], zoom_start=14)\n",
    "\n",
    "# # get location information for address\n",
    "# address = geocoder.osm('2920 Zoo Dr, San Diego, CA 92101')\n",
    "\n",
    "# # address latitude and longitude\n",
    "# address_latlng = [address.lat, address.lng]\n",
    "\n",
    "# # add marker to map\n",
    "# folium.Marker(address_latlng, popup='San Diego Zoo', tooltip='click').add_to(map_zoo)\n",
    "\n",
    "# # display map\n",
    "# map_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset_filtered[dataset_filtered.state=='Bihar']\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # multiple markers using dataframe\n",
    "# # there is an example below using apply function instead of loop\n",
    "# start_time = time.time()\n",
    "# # create map\n",
    "# map_locations = folium.Map(location=[38, 80], zoom_start=4)\n",
    "\n",
    "# # plot airport locations\n",
    "# for (index, row) in a.iterrows():\n",
    "#     folium.Marker(location=[row.loc['latitude'], row.loc['longitude']], \n",
    "#                   popup= row.loc['city'] + ' ' + row.loc['state'], \n",
    "#                   tooltip='click').add_to(map_locations)\n",
    "\n",
    "# total_time = time.time() - start_time\n",
    "# # display map    \n",
    "# map_locations\n",
    "# # print(\"The total time taken for computation - \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.city.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.latitude.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
